{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PlagiarismDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSUqjaNdaepx"
      },
      "source": [
        "Plagiarism Detection- Catching up cheaters using Language Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQhInN7VaSqR"
      },
      "source": [
        "# Simple Plagiarism Detection\n",
        "# @author: Kh\n",
        "\n",
        "# importing required Libraries\n",
        "import re\n",
        "from nltk.util import ngrams, pad_sequence, everygrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.lm import MLE, WittenBellInterpolated\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from scipy.ndimage import gaussian_filter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FN7rz3HaVLD"
      },
      "source": [
        "actual code begins here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntTlrhoKbAch"
      },
      "source": [
        "# Training data file\n",
        "train_data_file = \"\"\n",
        "\n",
        "# read training data\n",
        "with open(train_data_file) as f:\n",
        "    train_text = f.read().lower()\n",
        "\n",
        "# apply preprocessing (remove text inside square and curly brackets and rem punc)\n",
        "train_text = re.sub(r\"\\[.*\\]|\\{.*\\}\", \"\", train_text)\n",
        "train_text = re.sub(r'[^\\w\\s]', \"\", train_text)\n",
        "\n",
        "# set ngram number\n",
        "n = 4\n",
        "\n",
        "# pad the text and tokenize\n",
        "training_data = list(pad_sequence(word_tokenize(train_text), n, \n",
        "                                  pad_left=True, \n",
        "                                  left_pad_symbol=\"<s>\"))\n",
        "\n",
        "# generate ngrams\n",
        "ngrams = list(everygrams(training_data, max_len=n))\n",
        "print(\"Number of ngrams:\", len(ngrams))\n",
        "\n",
        "# build ngram language models\n",
        "model = WittenBellInterpolated(n)\n",
        "model.fit([ngrams], vocabulary_text=training_data)\n",
        "print(model.vocab)\n",
        "\n",
        "# testing data file\n",
        "test_data_file = \"\"\n",
        "\n",
        "# Read testing data\n",
        "with open(test_data_file) as f:\n",
        "    test_text = f.read().lower()\n",
        "test_text = re.sub(r'[^\\w\\s]', \"\", test_text)\n",
        "\n",
        "# Tokenize and pad the text\n",
        "testing_data = list(pad_sequence(word_tokenize(test_text), n, \n",
        "                                 pad_left=True,\n",
        "                                 left_pad_symbol=\"<s>\"))\n",
        "print(\"Length of test data:\", len(testing_data))\n",
        "\n",
        "# assign scores\n",
        "scores = []\n",
        "for i, item in enumerate(testing_data[n-1:]):\n",
        "    s = model.score(item, testing_data[i:i+n-1])\n",
        "    scores.append(s)\n",
        "\n",
        "scores_np = np.array(scores)\n",
        "\n",
        "# set width and height\n",
        "width = 8\n",
        "height = np.ceil(len(testing_data)/width).astype(\"int32\")\n",
        "print(\"Width, Height:\", width, \",\", height)\n",
        "\n",
        "# copy scores to rectangular blank array\n",
        "a = np.zeros(width*height)\n",
        "a[:len(scores_np)] = scores_np\n",
        "diff = len(a) - len(scores_np)\n",
        "\n",
        "# apply gaussian smoothing for aesthetics\n",
        "a = gaussian_filter(a, sigma=1.0)\n",
        "\n",
        "# reshape to fit rectangle\n",
        "a = a.reshape(-1, width)\n",
        "\n",
        "# format labels\n",
        "labels = [\" \".join(testing_data[i:i+width]) for i in range(n-1, len(testing_data), width)]\n",
        "labels_individual = [x.split() for x in labels]\n",
        "labels_individual[-1] += [\"\"]*diff\n",
        "labels = [f\"{x:60.60}\" for x in labels]\n",
        "\n",
        "# create heatmap\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "                z=a, x0=0, dx=1,\n",
        "                y=labels, zmin=0, zmax=1,\n",
        "                customdata=labels_individual,\n",
        "                hovertemplate='%{customdata} <br><b>Score:%{z:.3f}<extra></extra>',\n",
        "                colorscale=\"burg\"))\n",
        "fig.update_layout({\"height\":height*28, \"width\":1000, \"font\":{\"family\":\"Courier New\"}})\n",
        "fig['layout']['yaxis']['autorange'] = \"reversed\"\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}